# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iPCupga5gGM46wwbSmvrArI3BZ5XvI94
"""

import numpy as np
import tensorflow as tf

# 1) Алгоритм обучения методом градиентного спуска
# Функция активации (сигмоида)
def sigmoid(z):
    z_safe = np.clip(z, -500, 500)  # ограничение на входные значения сигмоиды
    return 1 / (1 + np.exp(-z_safe))

# Производная функции активации
def sigmoid_derivative(z):
    return sigmoid(z) * (1 - sigmoid(z))

# Шаг 1. Подготовить обучающую выборку- x_train: это входные признаки (матрица изображений). y_train: это метки (желаемые выходы).
#(x_train, y_train), (x_test, y_test) = mnist.load_data()
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()
print(x_train.shape) # форма набора данных

# Отфильтруем только классы "0" и "1"
data_train = np.where((y_train == 0) | (y_train == 1))[0]
x_train = x_train[data_train]
y_train = y_train[data_train]

# Аналогично для теста
data_test = np.where((y_test == 0) | (y_test == 1))[0]
x_test = x_test[data_test]
y_test = y_test[data_test]

# Нормализация данных приводит значения пикселей изображений (которые изначально находятся в диапазоне от 0 до 255) к диапазону от 0 до 1
x_train = x_train / 255.0
x_test = x_test / 255.0

# Эта операция преобразует 2D-изображения (28x28 пикселей) в векторы (784 пикселя) для каждого изображения.
# Преобразование изображений в векторы
x_train = x_train.reshape(x_train.shape[0], -1)
x_test = x_test.reshape(x_test.shape[0], -1)

# Добавляем смещение (bias) в виде дополнительной единицы
x_train = np.hstack([np.ones((len(x_train), 1)), x_train])
x_test = np.hstack([np.ones((len(x_test), 1)), x_test])

# 2 Инициализация весов и смещений
np.random.seed(1)
# Определение размеров входных и выходных данных
input_dim = x_train.shape[1] # Количество входов (784 для MNIST) # Размерность входных данных (785, включая смещение)
# Шаг 2. Инициализировать веса случайными значениями:Каждому синаптическому весу и смещению присваиваются маленькие случайные значения.
weights = np.random.randn(input_dim) * 0.01  # Инициализация весов

# Параметры обучения
learning_rate = 0.01 # величина шага, с которым обновляются веса модели во время обучения
epochs = 20
desired_accuracy = 0.95  # Порог точности для остановки
errors = []          # Список для записи ошибок

# Обучение методом градиентного спуска
for epoch in range(epochs):
    total_error = 0 # Ошибки суммируются за всю эпоху
    # Шаг 3. Присваивать начальной ошибке и изменениям весов нулевые значения:Перед началом каждой эпохи общий показатель ошибки сбрасывается в ноль.
    accumulated_delta = np.zeros_like(weights)  # Накопитель дельт для весов

    # Шаг 4: Подготовка входных данных
    # Проход по всем примерам выборки
    for i in range(x_train.shape[0]):

        # Входные данные и целевые значения
        X = x_train[i] # Входные данные (изображение). Обработка данных по одному примеру
        D = y_train[i] # Целевое значение (метка)

        # Шаг 5. Вычислить выход нейрона:
        # Рассчитывается выходное значение нейрона на основе взвешенной суммы и функции активации.
        a = np.dot(X, weights)      # Взвешенная сумма входов
        Y = sigmoid(a)          # Результат после сигмоиды
        # Вычисление ошибки
        # Шаг 6. Вычислить ошибку и обновить общую ошибку:Определяется разница между желаемым и предсказанным значением.
        error = D - Y           # Разница между настоящим и предсказанным значением
        #current_error += abs(error)  # Суммируем абсолютные ошибки

        total_error += np.sum(error**2) / 2  # Используем среднеквадратичную ошибку
        # Обновление весов (Gradient Descent)
        # Шаг 7. Определить величину коррекции весов:
        #Вычисляется градиент ошибки, исходя из которого будут изменены веса.
        delta = error * sigmoid_derivative(a)  # Коррекция на основе производной сигмоиды
        #weights += learning_rate * delta * X   # Обновляем веса
        accumulated_delta += delta * X  # Накапливаем изменения весов

    # Обновляем веса по результатам всей эпохи
    weights += learning_rate * accumulated_delta

    # Запись ошибки на текущей эпохе
    average_epoch_error = total_error / x_train.shape[0] # механизмом отслеживания среднего значения ошибки за эпоху
    errors.append(total_error)
    print(f'Эпоха {epoch+1}, Общая ошибка: {total_error:.5f}')

    # Ранняя остановка, если ошибка стала близкой к нулю
    #Шаг 8. Завершить текущую эпоху и проверить критерий останова:
    #После прохождения всей выборки (этап обучения завершился), проводится проверка достижения нужного уровня точности.
    if average_epoch_error <= 0.01:
        print("Обучение закончено, достаточная точность достигнута!")
        break

# Оценка модели на тестовых данных
correct_predictions = 0
for i in range(x_test.shape[0]):
    X_2 = x_test[i]
    D_2 = y_test[i]
    a = np.dot(X_2, weights)
    Y_2 = sigmoid(a)
    prediction = 1 if Y_2 >= 0.5 else 0
    if prediction == D_2:
        correct_predictions += 1

test_accuracy = correct_predictions / x_test.shape[0]
print(f'Точность на тестовых данных: {test_accuracy:.5f}')