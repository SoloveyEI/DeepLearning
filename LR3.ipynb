{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Импорт необходимых библиотек\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "l0lwGpaGbAJJ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем данные\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Изображения имеют размер 28x28, следовательно, являются двухмерными. Поскольку наш персептрон\n",
        "# способен считывать только одномерные данные, преобразуем их / Предобработка данных\n",
        "x_train = x_train.reshape(x_train.shape[0], -1) / 255.0\n",
        "x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Архитектура 1/ Опишем архитектуру сети\n",
        "model = Sequential() # создается последовательная модель нейронной сети\n",
        "model.add(Dense(10, input_dim=784, activation='relu')) # добавляется скрытый слой с 10 нейронами и функцией активации ReLU, входная размерность указана в параметре input_dim\n",
        "model.add(Dense(10, activation='softmax')) # добавляется выходной слой с 10 нейронами и функцией активации softmax для многоклассовой классификации\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # модель компилируется с функцией потерь categorical_crossentropy, оптимизатором Adam и метрикой точности\n",
        "# Начнем обучение/модель обучается на данных x_train и y_train на протяжении 10 эпох, с использованием параметра validation_split для автоматического разделения данных на валидационные\n",
        "model.fit(x_train, y_train, epochs=10, validation_split=0.1)\n",
        "\n",
        "# Проанализируйте результат на проверочных и тестовых данных\n",
        "_, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f'Модель 1: {test_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zuP3V60Iblno",
        "outputId": "56f69f0d-21fa-4a34-9f97-4e3427059cd6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.6708 - loss: 0.9745 - val_accuracy: 0.8265 - val_loss: 0.5176\n",
            "Epoch 2/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8303 - loss: 0.4989 - val_accuracy: 0.8393 - val_loss: 0.4672\n",
            "Epoch 3/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8428 - loss: 0.4600 - val_accuracy: 0.8477 - val_loss: 0.4495\n",
            "Epoch 4/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8533 - loss: 0.4317 - val_accuracy: 0.8398 - val_loss: 0.4568\n",
            "Epoch 5/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8557 - loss: 0.4230 - val_accuracy: 0.8523 - val_loss: 0.4302\n",
            "Epoch 6/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8579 - loss: 0.4135 - val_accuracy: 0.8463 - val_loss: 0.4462\n",
            "Epoch 7/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8609 - loss: 0.4055 - val_accuracy: 0.8552 - val_loss: 0.4222\n",
            "Epoch 8/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8634 - loss: 0.3970 - val_accuracy: 0.8597 - val_loss: 0.4211\n",
            "Epoch 9/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.8637 - loss: 0.3905 - val_accuracy: 0.8603 - val_loss: 0.4206\n",
            "Epoch 10/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8673 - loss: 0.3837 - val_accuracy: 0.8532 - val_loss: 0.4330\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8411 - loss: 0.4453\n",
            "Модель 1: 0.8382999897003174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Опишем архитектуру сети 2\n",
        "model2 = Sequential() # создается последовательная модель нейронной сети\n",
        "model2.add(Dense(50, input_dim=784, activation='relu')) # добавляется скрытый слой с 50 нейронами, функцией активации ReLU и входной размерностью 784\n",
        "model2.add(Dense(10, activation='softmax')) # добавляется выходной слой с 10 нейронами и функцией активации softmax для многоклассовой классификации\n",
        "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # модель компилируется с функцией потерь categorical_crossentropy, оптимизатором Adam и метрикой точности\n",
        "\n",
        "model2.fit(x_train, y_train, epochs=10, validation_split=0.1) # модель обучается на данных x_train и y_train на протяжении 10 эпох, с использованием 10% данных для валидации\n",
        "\n",
        "# Проанализируйте результат на проверочных и тестовых данных\n",
        "_, test_acc2 = model2.evaluate(x_test, y_test)\n",
        "print(f'Модель 2: {test_acc2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGxm-ey0bxFY",
        "outputId": "22e78627-452c-4064-dc05-d37695eddf4c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7551 - loss: 0.7187 - val_accuracy: 0.8405 - val_loss: 0.4366\n",
            "Epoch 2/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8504 - loss: 0.4189 - val_accuracy: 0.8602 - val_loss: 0.3808\n",
            "Epoch 3/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8669 - loss: 0.3782 - val_accuracy: 0.8677 - val_loss: 0.3701\n",
            "Epoch 4/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8733 - loss: 0.3499 - val_accuracy: 0.8658 - val_loss: 0.3789\n",
            "Epoch 5/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8824 - loss: 0.3290 - val_accuracy: 0.8712 - val_loss: 0.3503\n",
            "Epoch 6/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8853 - loss: 0.3145 - val_accuracy: 0.8793 - val_loss: 0.3476\n",
            "Epoch 7/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8889 - loss: 0.3037 - val_accuracy: 0.8797 - val_loss: 0.3383\n",
            "Epoch 8/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8943 - loss: 0.2874 - val_accuracy: 0.8757 - val_loss: 0.3419\n",
            "Epoch 9/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8971 - loss: 0.2836 - val_accuracy: 0.8755 - val_loss: 0.3470\n",
            "Epoch 10/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9007 - loss: 0.2697 - val_accuracy: 0.8812 - val_loss: 0.3355\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8742 - loss: 0.3547\n",
            "Модель 2: 0.8723000288009644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Опишем архитектуру сети 3: Многослойный персептрон состоящий из двух скрытых слоев с 50 нейронами каждый\n",
        "model3 = Sequential()\n",
        "model3.add(Dense(50, input_dim=784, activation='relu')) # добавляется первый скрытый слой с 50 нейронами, функцией активации ReLU и входной размерностью 784\n",
        "model3.add(Dense(50, activation='relu')) # добавляется второй скрытый слой с 50 нейронами и функцией активации ReLU\n",
        "model3.add(Dense(10, activation='softmax')) # добавляется выходной слой с 10 нейронами и функцией активации softmax для многоклассовой классификации\n",
        "model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model3.fit(x_train, y_train, epochs=10, validation_split=0.1) # модель обучается на данных xtrain и ytrain на протяжении 10 эпох, с использованием 10% данных для валидации\n",
        "\n",
        "_, test_acc3 = model3.evaluate(x_test, y_test)\n",
        "print(f'Модель 3: {test_acc3}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maYyBE-Sbw06",
        "outputId": "c717ce63-80a9-4069-c3eb-bb6487ad9986"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7568 - loss: 0.7086 - val_accuracy: 0.8468 - val_loss: 0.4354\n",
            "Epoch 2/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8538 - loss: 0.4075 - val_accuracy: 0.8605 - val_loss: 0.3824\n",
            "Epoch 3/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8724 - loss: 0.3537 - val_accuracy: 0.8595 - val_loss: 0.3750\n",
            "Epoch 4/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8777 - loss: 0.3348 - val_accuracy: 0.8608 - val_loss: 0.3760\n",
            "Epoch 5/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8840 - loss: 0.3127 - val_accuracy: 0.8742 - val_loss: 0.3435\n",
            "Epoch 6/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8888 - loss: 0.2993 - val_accuracy: 0.8790 - val_loss: 0.3498\n",
            "Epoch 7/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8943 - loss: 0.2866 - val_accuracy: 0.8767 - val_loss: 0.3386\n",
            "Epoch 8/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8988 - loss: 0.2746 - val_accuracy: 0.8793 - val_loss: 0.3467\n",
            "Epoch 9/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8990 - loss: 0.2671 - val_accuracy: 0.8767 - val_loss: 0.3476\n",
            "Epoch 10/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9035 - loss: 0.2570 - val_accuracy: 0.8803 - val_loss: 0.3526\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8714 - loss: 0.3616\n",
            "Модель 3: 0.8726999759674072\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загружаем данные\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Подготовка данных для CNN\n",
        "x_train_cnn = x_train[:,:,:,np.newaxis] / 255.0 # изменяется форма x_train, добавляя размерность для канала, и нормализуются данные, деля на 255.0\n",
        "x_test_cnn = x_test[:,:,:,np.newaxis] / 255.0 # изменяется форма x_test и нормализуются данные\n",
        "y_train_cnn = to_categorical(y_train) # преобразуются метки y_train в бинарный формат\n",
        "y_test_cnn = to_categorical(y_test) # преобразуются метки y_test\n",
        "\n",
        "# Опишем архитектуру сети 4: Сверточная нейронная сеть\n",
        "model4 = Sequential()\n",
        "# добавляется сверточный слой с 64 фильтрами, размером ядра 2x2, с поддержкой границ (padding='same') и функцией активации ReLU\n",
        "model4.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(28,28, 1)))\n",
        "model4.add(MaxPooling2D(pool_size=2)) # добавляется слой подвыборки (max pooling) с размером 2x2 для уменьшения размерности\n",
        "model4.add(Flatten()) # преобразует многомерный выход из предыдущего слоя в одномерный вектор\n",
        "model4.add(Dense(10, activation='softmax')) # добавляется выходной слой с 10 нейронами и функцией активации softmax\n",
        "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model4.fit(x_train_cnn, y_train_cnn, epochs=10, validation_split=0.1)\n",
        "\n",
        "_, test_acc4 = model4.evaluate(x_test_cnn, y_test_cnn)\n",
        "print(f'Модель 4: {test_acc4}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-_Ou10Mb4Xx",
        "outputId": "2aa6d61a-e7c8-4ee4-aed1-16540c15d16a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7964 - loss: 0.5821 - val_accuracy: 0.8777 - val_loss: 0.3417\n",
            "Epoch 2/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8878 - loss: 0.3190 - val_accuracy: 0.8903 - val_loss: 0.3081\n",
            "Epoch 3/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9034 - loss: 0.2761 - val_accuracy: 0.8983 - val_loss: 0.2891\n",
            "Epoch 4/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9110 - loss: 0.2531 - val_accuracy: 0.8987 - val_loss: 0.2825\n",
            "Epoch 5/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9186 - loss: 0.2300 - val_accuracy: 0.8982 - val_loss: 0.2820\n",
            "Epoch 6/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9181 - loss: 0.2267 - val_accuracy: 0.8992 - val_loss: 0.2849\n",
            "Epoch 7/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9242 - loss: 0.2125 - val_accuracy: 0.9042 - val_loss: 0.2753\n",
            "Epoch 8/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9272 - loss: 0.2011 - val_accuracy: 0.9078 - val_loss: 0.2655\n",
            "Epoch 9/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9324 - loss: 0.1888 - val_accuracy: 0.9025 - val_loss: 0.2714\n",
            "Epoch 10/10\n",
            "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9333 - loss: 0.1850 - val_accuracy: 0.9062 - val_loss: 0.2700\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9020 - loss: 0.2781\n",
            "Модель 4: 0.9013000130653381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Сравнение результатов всех моделей\n",
        "print(f'Точность архитектуры 1: {test_acc}')\n",
        "print(f'Точность архитектуры 2: {test_acc2}')\n",
        "print(f'Точность архитектуры 3: {test_acc3}')\n",
        "print(f'Точность архитектуры 4 (CNN): {test_acc4}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n58PAyG9b9dY",
        "outputId": "d5134f18-136b-4b2f-ea05-2dae95111e27"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность архитектуры 1: 0.8382999897003174\n",
            "Точность архитектуры 2: 0.8723000288009644\n",
            "Точность архитектуры 3: 0.8726999759674072\n",
            "Точность архитектуры 4 (CNN): 0.9013000130653381\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вариант 3\n",
        "Архитектура модели: 2-3-3\n",
        "Скорость обучения: 0.25\n",
        "Значения X={0.4;-0.4}\n",
        "Значения Y={0.3;-0.5;0.8}\n",
        "Начальные веса 1 слоя W1 произвольным образом из интервала [-0.3 0.3]\n",
        "Начальные веса 2 слоя W2 произвольным образом из интервала [-0.3 0.3]\n",
        "\n",
        "Входной слой:\n",
        "Размерность входного слоя: 2 нейрона (для входного вектора X = {0.4, -0.4}).\n",
        "\n",
        "Скрытые слои:\n",
        "- Первый скрытый слой: 3 нейрона с функцией активации логистического сигмоида.\n",
        "- Второй скрытый слой: 3 нейрона с функцией активации логистического сигмоида.\n",
        "\n",
        "Выходной слой:\n",
        "- Размерность выходного слоя: 3 нейрона (для эталонного выхода Y = {0.3, 0.5, 0.8}) с функцией активации логистического сигмоида.\n",
        "\n",
        "Функция потерь:\n",
        "- Используется среднеквадратичная ошибка (MSE) для оценки производительности.\n",
        "\n",
        "Оптимизация:\n",
        "- Используется алгоритм обратного распространения ошибки для корректировки весов, скорость обучения: 0.25."
      ],
      "metadata": {
        "id": "_d9JezP2cBjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "import random\n",
        "\n",
        "# Входные данные и целевое значение\n",
        "X = np.array([[0.4, -0.4]]) # Входные данные\n",
        "Y = np.array([[0.3, -0.5, 0.8]]) # Желаемые выходные значения\n",
        "\n",
        "# Инициализация весов\n",
        "W1 = np.random.uniform(-0.3, 0.3, (2, 3)) # 2 входа, 3 выхода /Случайные веса первого слоя\n",
        "W2 = np.random.uniform(-0.3, 0.3, (3, 3)) # 3 входа, 3 выхода /Случайные веса второго слоя\n",
        "\n",
        "# Определение оптимизатора\n",
        "optimizer = Adam(learning_rate=0.25)\n",
        "\n",
        "# Создание модели\n",
        "model = Sequential()\n",
        "model.add(Dense(3, input_dim=2, activation='sigmoid', use_bias=False))\n",
        "model.add(Dense(3, activation='sigmoid', use_bias=False))\n",
        "\n",
        "print(model.layers) # Вывод слоев модели\n",
        "\n",
        "# Установка начальных весов для слоев\n",
        "model.layers[0].set_weights([W1]) # Веса первого слоя\n",
        "model.layers[1].set_weights([W2]) # Веса второго слоя\n",
        "model.compile(loss='mse', optimizer=optimizer) # Компиляция модели\n",
        "model.fit(X, Y, epochs=100, verbose=2) # Обучение модели\n",
        "\n",
        "loss = model.evaluate(X, Y) # Оценка модели\n",
        "\n",
        "pred = model.predict(X)\n",
        "print(f\"Predicted value: {pred[0][0]:.1f}, {pred[0][1]:.1f}, {pred[0][2]:.1f}\")"
      ],
      "metadata": {
        "id": "iG3IcnqVfLra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Контрольные вопросы:\n",
        "1. Понятие нейронной сети, архитектуры\n",
        "\n",
        "Нейронная сеть — это вычислительная модель, имитирующая организацию нервной системы живого организма. Она представляет собой совокупность узлов (нейронов), объединенных в слои. Каждый нейрон получает входящие сигналы, производит обработку и передает результат дальше.\n",
        "\n",
        "Архитектура нейронной сети описывает внутреннюю структуру модели, включая число слоев, количество нейронов в каждом слое, вид соединений между ними и используемые функции активации. Среди наиболее известных архитектур выделяют:\n",
        "\n",
        "• Многослойный перцептрон (Multilayer Perceptron, MLP): простая структура с несколькими полносвязанными слоями, широко применяется для задач классификации и регрессии.\n",
        "• Сверточные нейронные сети (Convolutional Neural Networks, CNN): специализированные сети для обработки изображений и пространственных данных, содержащие свёрточные и субдискретизационные слои.\n",
        "• Рекуррентные нейронные сети (Recurrent Neural Networks, RNN): эффективны для работы с временными рядами и последовательностными данными, обладают механизмом памяти, позволяющим учитывать порядок поступления данных.\n",
        "\n",
        "2. Обучение нейронной сети\n",
        "\n",
        "Обучение нейронной сети заключается в автоматической настройке весов и смещений, чтобы минимизировать расхождение между результатами, выдаваемыми моделью, и реальными значениями целевой переменной. В ходе обучения сеть итеративно улучшает свою способность давать точные прогнозы, постоянно корректируя внутренние параметры на основе ошибки, вычисленной между прогнозированными и истинными значениями.\n",
        "\n",
        "3. Основные определения: скорость обучения, эпоха, нейрон,\n",
        "обучающая выборка, тестовая выборка, вариационная выборка, функция активации.\n",
        "\n",
        "• Скорость обучения (Learning Rate) – параметр, определяющий величину шага при обновлении весов во время обучения. Слишком высокая скорость обучения может привести к нестабильности, а слишком низкая – к медленной сходимости.\n",
        "\n",
        "• Эпоха (Epoch) – полное прохождение всех примеров обучающего набора данных через сеть.\n",
        "\n",
        "• Нейрон – элементарный узел сети, выполняющий простое преобразование входящего сигнала.\n",
        "\n",
        "• Обучающая выборка (Training set) — набор данных, используемый для обучения сети.\n",
        "\n",
        "• Тестовая выборка (Test set) — независимый набор данных для оценки качества обученной сети.\n",
        "\n",
        "• Вариационная выборка (Validation Set) – набор данных, используемый для настройки гиперпараметров нейронной сети и предотвращения переобучения.\n",
        "\n",
        "• Функция активации – нелинейная функция, применяемая к нейрону для преобразования входного сигнала.\n",
        "\n",
        "4. Алгоритм обратного распространения ошибки\n",
        "\n",
        "Алгоритм обратного распространения ошибки (Backpropagation) — это ключевой метод, используемый для обучения нейронных сетей посредством корректировки весов, основываясь на градиенте функции потерь. Основная идея заключается в следующем:\n",
        "\n",
        "Прямое распространение: Входные данные проходят через нейронную сеть, генерируя прогнозируемые значения на выходе. Каждая связь между нейронами имеет определённый вес, который влияет на передачу сигнала далее по сети.\n",
        "\n",
        "Вычисление ошибки: Определяется разница между ожидаемым (цельным) значением и прогнозом, полученной на выходе сети. Измеряется с помощью функции потерь (например, среднеквадратической ошибки MSE или кросс-энтропии).\n",
        "\n",
        "Обратное распространение: Ошибка распространяется обратно по сети, начиная с выходного слоя. На каждом слое вычисляются градиенты ошибки относительно весов соединений.\n",
        "\n",
        "Обновление весов: Веса обновляются с использованием градиентов и скорости обучения, чтобы уменьшить ошибку.\n",
        "\n",
        "5. Типы функций активации\n",
        "\n",
        "• Сигмоида (Sigmoid): Нелинейная функция, ограничивающая значения в пределах от 0 до 1. Чаще всего применяется в задачах бинарной классификации и может служить пороговым элементом в моделях.\n",
        "• ReLU (Rectified Linear Unit): Простая кусочно-линейная функция, возвращающая 0 для отрицательных аргументов и сам аргумент для неотрицательных. Является одной из самых распространенных функций активации в глубоком обучении, особенно в скрытых слоях.\n",
        "\n",
        "• Tanh (Гиперболический тангенс):Функция, похожая на сигмоиду, но возвращает значения в диапазоне от −1 до 1. Хорошо центрирует данные вокруг нуля, улучшая сходимость градиентного спуска.\n",
        "\n",
        "• Softmax: Преобразует вектор вещественных чисел в нормальное распределение вероятностей, суммирующихся к 1. Обычно используется в финальном слое для задач многоклассового деления, когда нужно интерпретировать результаты как вероятности принадлежности к различным классам.\n",
        "\n",
        "6. Алгоритмы обучения\n",
        "\n",
        "• Градиентный спуск (Gradient Descent):Классический метод оптимизации, при котором веса нейронной сети обновляются одновременно после просмотра всей обучающей выборки (полная обработка всей партии данных). Новый набор весов выбирается в сторону уменьшения градиента функции потерь.\n",
        "\n",
        "• Стохастический градиентный спуск (Stochastic Gradient Descent, SGD):Обновление весов происходит после обработки каждого индивидуального примера из обучающей выборки.\n",
        "\n",
        "• Мини-пакетный градиентный спуск (Mini-batch Gradient Descent):Веса обновляются после обработки небольшой группы («мини-пакета») примеров данных. Это балансирует быстроту вычислений и снижение шума, возникающего при стохастическом подходе.\n",
        "\n",
        "• RMSprop (Root Mean Square Propagation):Ещё один адаптивный алгоритм, который регулирует скорость обучения на основе средней величины квадрата предыдущих градиентов. Экспоненциально усредняясь, он предотвращает резкие скачки и стабилизирует движение к минимуму.\n",
        "\n",
        "• Adam (Adaptative Moment Estimation):Эффективный адаптивный алгоритм, комбинирующий преимущества Momentum и адаптивных скоростей обучения. Он накапливает экспоненциально сглаженные оценки среднего и дисперсии градиентов, обеспечивая высокую скорость и надежность обучения."
      ],
      "metadata": {
        "id": "GJ4Jn8rBcRlj"
      }
    }
  ]
}